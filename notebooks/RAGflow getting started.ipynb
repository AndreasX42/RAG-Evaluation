{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f077f9-cb9d-4b61-b3a9-72bcb2fd57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Provide API Keys in .env file\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(), override=True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../backend/\")\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f3a75-33c2-49fc-abdc-e7baa98163fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53cbb70a-d5d9-4489-896a-42b8f92ac43b",
   "metadata": {},
   "source": [
    "# How do generated QA pairs look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d856ff8-ce98-44de-9cde-73e3ae72efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../resources/dev/label_dataset.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    qa_eval_pairs = json.load(file)\n",
    "\n",
    "# each QA pair contains generated question, answer, context in document with corresponding source and attached id\n",
    "gt_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bfffe-9eb8-41ad-945c-063ec9f42b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc0d927-ecba-41fc-8378-35de3fab0d55",
   "metadata": {},
   "source": [
    "# Load and split data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9c062-57f2-4bca-bf89-1b039a1293fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.utils import load_and_chunk_doc, get_qa_llm, get_retriever\n",
    "from backend.commons.configurations import Hyperparameters, CVRetrieverSearchType\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import uuid\n",
    "\n",
    "user_id = str(uuid.uuid4())\n",
    "\n",
    "hp_dict = {\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 10,\n",
    "        \"num_retrieved_docs\": 3,\n",
    "        \"similarity_method\": \"cosine\",\n",
    "        \"search_type\": \"mmr\",\n",
    "        \"length_function_name\": \"text-embedding-ada-002\",\n",
    "        \"embedding_model\": \"text-embedding-ada-002\",\n",
    "        \"qa_llm\": \"gpt-3.5-turbo\",\n",
    "        # settings for grading llm\n",
    "        \"use_llm_grader\": False,\n",
    "        #\"grade_answer_prompt\": \"few_shot\",\n",
    "        #\"grade_docs_prompt\": \"default\",\n",
    "        #\"grader_llm\": \"gpt-3.5-turbo\"\n",
    "    }\n",
    "\n",
    "api_keys = {\"OPENAI_API_KEY\": os.environ.get(\"OPENAI_API_KEY\")}\n",
    "\n",
    "hp = Hyperparameters.from_dict(input_dict=hp_dict, hp_id=99, api_keys=api_keys)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "emb = OpenAIEmbeddings()\n",
    "\n",
    "chunks = load_and_chunk_doc(hp, glob.glob(\"../resources/tests/document_store/*.txt\")[0])\n",
    "\n",
    "# creates chroma collection for user and hp_id and embedds document chunks\n",
    "retriever = get_retriever(chunks=chunks, hp=hp, user_id=user_id)\n",
    "qa_llm = get_qa_llm(retriever=retriever, qa_llm=llm, return_source_documents=True)\n",
    "\n",
    "print(f\"number of tokens in document: {sum([llm.get_num_tokens(chunk.page_content) for chunk in chunks])}\\\n",
    "\\nnumber of chunks: {len(chunks)}\\\n",
    "\\naverage number of tokens per chunk: {np.average([llm.get_num_tokens(chunk.page_content) for chunk in chunks])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204176c-5061-476b-9e16-be3d135b3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_llm(\"Of what time does the document speak of?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fa93a-a416-4928-b559-39964ebcdbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "864a584f-ecab-45be-815a-acc3c515c3f6",
   "metadata": {},
   "source": [
    "# Generate Q/A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b0d23-abb7-46d3-b944-469a3fec767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.commons.configurations import BaseConfigurations, QAConfigurations, Hyperparameters\n",
    "from langchain.chains import QAGenerationChain\n",
    "from backend.commons.prompts import QA_GENERATION_PROMPT_SELECTOR\n",
    "from backend.generation.label_dataset_generator import get_qa_from_chunk\n",
    "import itertools\n",
    "\n",
    "qa_params = {\n",
    "        \"chunk_size\": 2048,\n",
    "        \"chunk_overlap\": 0,\n",
    "        \"qa_generator_llm\": \"gpt-3.5-turbo\",\n",
    "        \"length_function_name\": \"len\", # measures the chunk_size with this length function, could also be number of tokens form embedding model\n",
    "        \"persist_to_vs\": True, # also embedds generated answers for caching and later evaluation\n",
    "        \"embedding_model_list\": [\"text-embedding-ada-002\"]\n",
    "    }\n",
    "\n",
    "hp_qa = QAConfigurations.from_dict(input_dict=qa_params, api_keys=api_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5eb66-6c64-44b8-8056-36a5ee4aec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = QAGenerationChain.from_llm(hp_qa.qa_generator_llm, prompt=QA_GENERATION_PROMPT_SELECTOR.get_prompt(hp_qa.qa_generator_llm))\n",
    "\n",
    "qa_pairs = [await get_qa_from_chunk(chunks[i], qa_chain) for i in range(1)]\n",
    "qa_pairs = list(itertools.chain.from_iterable(qa_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e318d63-1f79-42d0-95c3-176831f93c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one question-answer-context triple from first document chunk\n",
    "qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4a92b-1c5e-4cb5-8ac9-56a45fb89962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787afb1-402a-4b38-bbdc-e09877420ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ac142f-b2a8-4796-ba06-c6d1ff31bf3a",
   "metadata": {},
   "source": [
    "### Grading metrics like embedding similarity can be found in the backend.evaluation.metrics module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
