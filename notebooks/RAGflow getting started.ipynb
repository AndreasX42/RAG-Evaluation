{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f077f9-cb9d-4b61-b3a9-72bcb2fd57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(), override=True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../backend/\")\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f3a75-33c2-49fc-abdc-e7baa98163fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53cbb70a-d5d9-4489-896a-42b8f92ac43b",
   "metadata": {},
   "source": [
    "# How do generated QA pairs look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d856ff8-ce98-44de-9cde-73e3ae72efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../tmp/eval_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Load existing data into a list\n",
    "    gt_dataset = json.load(file)\n",
    "\n",
    "# each QA pair contains generated question, answer, context in document with corresponding source and attached id\n",
    "gt_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3165d8a-b4a2-4ead-8180-920eed4ec093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47f7c0-9788-4b68-8d84-1cb835a1ab59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc0d927-ecba-41fc-8378-35de3fab0d55",
   "metadata": {},
   "source": [
    "# Load and split data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9c062-57f2-4bca-bf89-1b039a1293fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.utils import load_and_chunk_doc, get_qa_llm, get_retriever\n",
    "from backend.commons.configurations import Hyperparameters, CVRetrieverSearchType\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "hp_dict = {\n",
    "        \"id\": 0,\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 10,\n",
    "        \"num_retrieved_docs\": 3,\n",
    "        \"use_llm_grader\": False,\n",
    "        \"search_type\": \"mmr\",\n",
    "        \"length_function_name\": \"text-embedding-ada-002\",\n",
    "        \"grade_answer_prompt\": \"few_shot\",\n",
    "        \"grade_docs_prompt\": \"default\",\n",
    "        \"embedding_model\": \"text-embedding-ada-002\",\n",
    "        \"qa_llm\": \"gpt-3.5-turbo\",\n",
    "        \"grader_llm\": \"gpt-3.5-turbo\"\n",
    "    }\n",
    "\n",
    "hp = Hyperparameters.from_dict(hp_dict)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "emb = OpenAIEmbeddings()\n",
    "\n",
    "chunks = load_and_chunk_doc(hp, glob.glob(\"../tmp/document_store/*.pdf\")[1])\n",
    "retriever = get_retriever(chunks,emb,3, search_type=CVRetrieverSearchType.MMR)\n",
    "qa_llm = get_qa_llm(retriever, llm)\n",
    "\n",
    "print(f\"number of tokens in document: {sum([llm.get_num_tokens(chunk.page_content) for chunk in chunks])}\\\n",
    "\\nnumber of chunks: {len(chunks)}\\\n",
    "\\naverage number of tokens per chunk: {np.average([llm.get_num_tokens(chunk.page_content) for chunk in chunks])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ece702-3173-43f6-ab7b-3d1d83c28ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "864a584f-ecab-45be-815a-acc3c515c3f6",
   "metadata": {},
   "source": [
    "# Generate Q/A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b0d23-abb7-46d3-b944-469a3fec767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.commons.configurations import BaseConfigurations, QAConfigurations, Hyperparameters\n",
    "from langchain.chains import QAGenerationChain\n",
    "from backend.commons.prompts import QA_GENERATION_PROMPT_SELECTOR\n",
    "from backend.testsetgen.test_set_generator import get_qa_from_chunk\n",
    "import itertools\n",
    "\n",
    "qa_params = {\n",
    "        \"chunk_size\": 2048,\n",
    "        \"chunk_overlap\": 0,\n",
    "        \"qa_generator_llm\": \"gpt-3.5-turbo\",\n",
    "        \"length_function_name\": \"text-embedding-ada-002\",\n",
    "        \"generate_eval_set\": True,\n",
    "        \"persist_to_vs\": True,\n",
    "        \"embedding_model_list\": [\"text-embedding-ada-002\",\"text-embedding-ada-002\"]\n",
    "    }\n",
    "\n",
    "hp_qa = QAConfigurations.from_dict(qa_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5eb66-6c64-44b8-8056-36a5ee4aec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "qa_chain = QAGenerationChain.from_llm(hp_qa.qa_generator_llm, prompt=QA_GENERATION_PROMPT_SELECTOR.get_prompt(hp_qa.qa_generator_llm))\n",
    "\n",
    "qa_pairs = [await get_qa_from_chunk(chunks[i], qa_chain) for i in range(len(chunks))]\n",
    "qa_pairs = list(itertools.chain.from_iterable(qa_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90581f7-84ad-452a-8207-07ea2b4c9567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dffded25-d199-494c-9551-aa3cd15ec38b",
   "metadata": {},
   "source": [
    "# Set up vectorstore and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0ca75-0ba1-4b9b-b4b8-503a156e320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_backend.utils import get_retriever\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "chunks_vs = split_data(data=data, chunk_size=512)\n",
    "retriever = get_retriever(chunks_vs, OpenAIEmbeddings(model=\"text-embedding-ada-002\"), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef61adc-9ac3-4b8e-9a79-44d1a8955a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "025a103f-9c06-4866-ae87-aa8f2ac86058",
   "metadata": {},
   "source": [
    "# LLM chain for query answering based on document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9a0fb-899c-4a9f-bbbf-ea4d842b9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.utils import get_qa_llm\n",
    "qa_llm = get_qa_llm(retriever, hp.qa_llm)\n",
    "\n",
    "# also returns source document chunks by default\n",
    "await qa_llm.acall(\"What is TRAIL.X?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4a92b-1c5e-4cb5-8ac9-56a45fb89962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ac142f-b2a8-4796-ba06-c6d1ff31bf3a",
   "metadata": {},
   "source": [
    "QA grading functions like embedding similarity can be found in backend.evaluation.evaluation_metrics.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
