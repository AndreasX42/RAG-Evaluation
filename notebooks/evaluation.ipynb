{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f077f9-cb9d-4b61-b3a9-72bcb2fd57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(), override=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Callable\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../eval_backend/\")\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(Path.cwd())\n",
    "\n",
    "from eval_backend.utils import *\n",
    "\n",
    "import json\n",
    "with open(\"./eval_set.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Load existing data into a list\n",
    "    gt_dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0d927-ecba-41fc-8378-35de3fab0d55",
   "metadata": {},
   "source": [
    "# load and split data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b9c062-57f2-4bca-bf89-1b039a1293fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens in document: 45390\n",
      "number of elements in data: 1\n",
      "number of chunks: 14\n",
      "average number of tokens per chunk: 3242.1428571428573\n"
     ]
    }
   ],
   "source": [
    "data = load_document(\"../resources/msg_life-gb-2021-EN_final.pdf\")\n",
    "chunks = split_data(data=data, chunk_size=3400)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(f\"number of tokens in document: {sum([llm.get_num_tokens(chunk.page_content) for chunk in chunks])}\\\n",
    "\\nnumber of elements in data: {len(data)}\\\n",
    "\\nnumber of chunks: {len(chunks)}\\\n",
    "\\naverage number of tokens per chunk: {np.average([llm.get_num_tokens(chunk.page_content) for chunk in chunks])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a584f-ecab-45be-815a-acc3c515c3f6",
   "metadata": {},
   "source": [
    "# Generate Q/A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e718144d-8fa6-41e3-8fbf-467cf93cc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a Q/A pair for a document chunk\n",
    "# result = generate_eval_set(llm, chunks)\n",
    "# from eval_backend.utils import write_json\n",
    "# write_json(gt_dataset, filename=\"./eval_set.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83baab1-7f3d-402f-9c2e-cf281c3e7fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2a43e-8d2e-4010-8068-e869f6a26cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa in gt_dataset:\n",
    "    if len(qa['answer'])>200:\n",
    "        print(qa[\"question\"])\n",
    "        print(f\"-> {qa['answer']}\")\n",
    "        print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffded25-d199-494c-9551-aa3cd15ec38b",
   "metadata": {},
   "source": [
    "# Set up vectorstore and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01af81-cbe7-48a1-a789-2d5d97df5b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c0ca75-0ba1-4b9b-b4b8-503a156e320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from eval_backend.utils import get_retriever\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "chunks_vs = split_data(data=data, chunk_size=512)\n",
    "retriever = get_retriever(chunks_vs, OpenAIEmbeddings(model=\"text-embedding-ada-002\"), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef61adc-9ac3-4b8e-9a79-44d1a8955a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "025a103f-9c06-4866-ae87-aa8f2ac86058",
   "metadata": {},
   "source": [
    "# LLM chain for query answering based on document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30f9a0fb-899c-4a9f-bbbf-ea4d842b9c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'hi',\n",
       " 'result': \"I'm sorry, but I don't have enough information to provide a helpful answer.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval_backend.utils import get_qa_llm\n",
    "qa_llm = get_qa_llm(retriever)#, retrieval_llm=ChatOpenAI(temperature=0, model=\"gpt-4\"))\n",
    "\n",
    "await qa_llm.acall(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7561288-a52e-4d27-be01-882bed8a260f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who are the members of the Management Board of msg life ag as of 31 December 2021?',\n",
       " 'answer': 'The members of the Management Board of msg life ag as of 31 December 2021 are Rolf Zielke (Chairman), Dr Aristid Neuburger (Deputy Chairman), Francesco Cargnel, Holger Gorissen, Robert Hess, Milenko Radic, Jens Stäcker, and Dr Wolf Wiedmann.',\n",
       " 'result': 'The members of the Management Board of msg life ag as of 31 December 2021 are Holger Gorissen, Robert Hess, and Jens Stäcker.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await qa_llm.acall(gt_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac142f-b2a8-4796-ba06-c6d1ff31bf3a",
   "metadata": {},
   "source": [
    "# QA_LLM grading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a486ab7-611b-4078-90f8-434e08c41a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_backend.eval_metrics import grade_model_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76dcbf2-0eed-4a82-a444-088da48e458e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54ff8d04-8b20-4878-9138-a30362f837d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are some of the challenges facing the German insurance industry?',\n",
       " 'answer': 'The digitisation of the economy and society, customer centricity and individualisation, industrialisation and automation, analytics and data effectiveness, and standardisation and integration.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04e3cb4e-09e3-4edc-b847-3e9685b8dbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are some of the challenges facing the German insurance industry?',\n",
       " 'result': 'Some of the challenges facing the German insurance industry include the difficult market conditions, the trend towards internationalization and consolidation, and the need for modern and flexible IT systems to increase efficiency and corporate success.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_llm_answer = qa_llm(gt_dataset[10][\"question\"])\n",
    "qa_llm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51d8646e-d3cc-4f80-bd03-f08c56cf899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = grade_model_answer([gt_dataset[10], gt_dataset[10]],\n",
    "                   [qa_llm_answer, qa_llm_answer],\n",
    "                   grade_answer_prompt=\"GPT4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16758f-262b-4e0c-8bb1-2d7cbe198e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79895031-2621-4ab4-a8a9-52fd794adef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72c6bf-a754-4d8d-a932-99ade64199a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd5f08-5a4d-440d-aef6-1c6d0f70a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for incorrect answer\n",
    "qa_predicted = qa_llm(\"What is the Life Factory?\")\n",
    "qa_predicted[\"question\"] = gt_dataset[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2246a-ad52-43b5-b04a-1766bf11e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd31d69-bad6-42ec-8a1f-ba6ee48732b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_model_answer([gt_dataset[0]],\n",
    "                   [qa_predicted],\n",
    "                   grade_answer_prompt=\"GPT4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1987b3-5e8b-4eac-bec8-7bcb172488e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a272a-8f22-41a8-b2c6-e52b03355872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad5ddee-cd16-4702-9013-1dc9918127c2",
   "metadata": {},
   "source": [
    "# Calculate quality of retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25f884e9-77c8-43d6-befb-6378a4accd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'results': 'GRADE: 4'}, {'results': 'GRADE: 4'}], 1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval_backend.eval_metrics import grade_model_retrieval\n",
    "\n",
    "qa_pair_index = 10\n",
    "query = gt_dataset[qa_pair_index][\"question\"]\n",
    "docs_retrieved = retriever.get_relevant_documents(gt_dataset[10][\"question\"])\n",
    "\n",
    "retrieved_doc_text = \"\\n\\n\".join(f\"Retrieved document {i}: {doc.page_content}\" for i, doc in enumerate(docs_retrieved))\n",
    "\n",
    "retrieved_dict = {\"question\": gt_dataset[qa_pair_index][\"question\"],\n",
    "             \"answer\": gt_dataset[qa_pair_index][\"answer\"], \"result\": retrieved_doc_text}\n",
    "\n",
    "grade_model_retrieval([gt_dataset[qa_pair_index],gt_dataset[qa_pair_index]],\n",
    "                      [retrieved_dict,retrieved_dict],\n",
    "                      grade_docs_prompt=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b7eaa-44a2-45c7-b608-163cd8e8ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dataset[qa_pair_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42739318-f34c-4410-b51a-902c4915036c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687bfc8-9186-4802-90ca-4fb28a58602f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "578ecd4a-dc18-429a-85ad-74d4f8b37b5a",
   "metadata": {},
   "source": [
    "# Calculate Embedding similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a9e48-2278-4324-b01d-94d8bd1ebf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_embedding_similarity(gt_dataset: List[Dict[str,str]], predictions: List[str],\n",
    "                                retriever: VectorStoreRetriever, embedding_model: Embeddings) -> List[float]:\n",
    "\n",
    "    m=len(gt_dataset)\n",
    "\n",
    "    target_embeddings = np.array(embedding_model.embed_documents([qa_pair[\"answer\"] for qa_pair in gt_dataset])).reshape(m, -1)\n",
    "    predicted_embeddings = np.array(embedding_model.embed_documents(predictions)).reshape(m, -1)\n",
    "\n",
    "    # similarities between openai embeddings ranges from 0.7 - 1.0 only\n",
    "    similarities = target_embeddings @ predicted_embeddings.T\n",
    "\n",
    "    return np.diag(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0d2f4-671b-4fcd-8cef-0fc0c71a5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_embedding_similarity([gt_dataset[0], gt_dataset[0]],\n",
    "                            [qa_llm.run(gt_dataset[0][\"question\"]), qa_llm.run(gt_dataset[15][\"question\"])],\n",
    "                            retriever=retriever,\n",
    "                            embedding_model=OpenAIEmbeddings(model=\"text-embedding-ada-002\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa625c8-74a0-4892-86e0-14a90967dc70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e6645-1508-4474-96c4-2a301c90e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_answer = gt_dataset[0][\"answer\"]\n",
    "pred_answer = qa_llm.run(gt_dataset[0][\"question\"])\n",
    "\n",
    "true_answer, pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeba613-fd3c-4188-ac2f-67cf6ad26086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "rouge_metric.compute(references=[true_answer], predictions=[pred_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd58b2-0a3c-44a3-ac0e-369980fbc088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcafc13e-c809-4ab2-86a8-0a1a07b06207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f1d1c-4f32-4ad2-a8e5-215fbfdd9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"my cat ate the cake\"]\n",
    "references = [\"my dog wants the cake too\"]\n",
    "results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754991b5-6303-4f94-8efe-346fdf6d0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * (3/6 * 3/5) / (3/6 + 3/5), 2 * (1/5 * 1/4) / (1/5 + 1/4), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5325be3-726a-4305-afa6-4cccdf5ec719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
