{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a83cd1-1956-4021-8df3-014a41872838",
   "metadata": {},
   "source": [
    "Brief overview how to use\n",
    "- SelfQueryRetriever\n",
    "- ParentDocumentRetriever\n",
    "- Hybrid Search with BM25Retriever and EnsembleRetriever\n",
    "- ContextualCompressionRetriever and Filters\n",
    "- MultiQueryRetriever\n",
    "- Cohere Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5025fa0-f69e-484a-8526-39c845f99fa2",
   "metadata": {},
   "source": [
    "## SelfQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939300c-daac-4936-8cd5-7ef1d99cdc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "df = pd.read_csv(\"data/wine_data.csv\")\n",
    "\n",
    "documents = list(df.apply(lambda row: Document(page_content=row[\"page_content\"],\n",
    "                    metadata=row[[\"name\", \"year\", \"rating\", \"grape\", \"color\", \"country\"]].to_dict()), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c2c9d-1d08-4685-bc50-f12aadf0eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c35763-d96e-4568-8575-5314e75d5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6bdcf-652e-4283-93f8-ba38e4ad7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(),override=True)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0306d1f3-3f8a-4394-9119-f68eca2865fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"grape\",\n",
    "        description=\"The grape used to make the wine\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"name\",\n",
    "        description=\"The name of the wine\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"color\",\n",
    "        description=\"The color of the wine\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the grapes where harvested.\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"country\",\n",
    "        description=\"The name of the country the wine comes from\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"The Robert Parker rating for the wine 0-100\", type=\"integer\" #float\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief description of the wine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651f33f-42d0-463c-824b-eb29bfe5d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    enable_limit=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3eeab2-0336-44d3-bf1b-58bd610f8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"I am looking for 3 white wines between 2010-2020 with ratings between 85-92\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c85ca6-ae13-49b1-bfcb-121e331682eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cad7c-a2c0-4eab-b2a3-fb729f765e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f347ce31-8d6e-4067-9f45-63e25b88e825",
   "metadata": {},
   "source": [
    "## ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a52f5-480b-45e0-a6fb-7dc587b6a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "## Text Splitting & Docloader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    #model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d410b2-360b-48e4-82cf-194b94463c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "loaders = [\n",
    "    TextLoader(doc, encoding=\"utf-8\") for doc in glob.glob(\"../resources/tests/document_store/*.txt\")\n",
    "]\n",
    "docs = []\n",
    "for l in loaders:\n",
    "    docs.extend(l.load())\n",
    "\n",
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=bge_embeddings)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "full_docs_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=store, \n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "full_docs_retriever.add_documents(docs, ids=None)\n",
    "\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b46551-4d04-4227-a43e-817a4081b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was Churchill thinking?\"\n",
    "sub_docs = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(\"\\n\\n\".join(sub_docs[i].page_content for i in range(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90970bd4-d357-4bb4-8313-3a09337042a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = full_docs_retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"\\n\\n\".join(retrieved_docs[i].page_content for i in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03af8c6-001e-4c4c-a68c-b6ff8be3c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(),override=True)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=full_docs_retriever)\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a5768-7a98-4de9-8f7a-8f11e72ab87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d96b4f40-5c54-4a1d-ad97-df7e97afb293",
   "metadata": {},
   "source": [
    "## Hybrid Search with BM25Retriever and EnsembleRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f9d2c-8625-4672-a1a9-c73f0c77af37",
   "metadata": {},
   "source": [
    "##### https://colab.research.google.com/drive/1lsT1V_U1Gq-jv09wv0ok5QHdyRjJyNxm?usp=sharing#scrollTo=Hv3UgdKiiuVr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52221ab-5a3c-4b31-bd51-f30d5069bd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1137d62a-4a12-47ff-acea-3d5a949f7919",
   "metadata": {},
   "source": [
    "## Contextual Compression and Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03b25d-ab5e-42a8-92d8-7cb6e476a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "## Text Splitting & Docloader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "import uuid\n",
    "import glob\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    #model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d94a3d-fd8b-4686-a562-5646a6076705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(docs):\n",
    "    print(f\"\\n{'-' * 50}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "    \n",
    "loaders = [\n",
    "    TextLoader(doc, encoding=\"utf-8\") for doc in glob.glob(\"../resources/tests/document_store/*.txt\")\n",
    "]\n",
    "docs = []\n",
    "for l in loaders:\n",
    "    docs.extend(l.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac73723-9289-4066-ad64-94bb5cd1e506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# collection_name has to be different in each .from_documents call, otherwise Chroma always uses default collection\n",
    "retriever = Chroma.from_documents(chunks, collection_name=str(uuid.uuid4()), embedding=bge_embeddings)\\\n",
    "                    .as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "query = \"What was Churchill thinking?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "#lets look at the docs\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfbed2e-e610-4247-bdaf-c81cc8a4a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(), override=True)\n",
    "\n",
    "# Creating the compressor\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# first stage will be usual retrieval with \"retriever\", second stage is to apply compressor\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
    "                                                       base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27ef1b-e4de-4961-a736-968ee2f199f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "\n",
    "pprint(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a28738-b6aa-40d6-a19f-9342213960ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(),override=True)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=compression_retriever)\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08818ab-7a62-42cf-98c6-0533916484b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering what document chunks should be passed to the LLM\n",
    "\n",
    "# 1. LLMChainFilter\n",
    "# https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html\n",
    "\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "filter = LLMChainFilter.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=filter, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "\n",
    "pprint(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b9ac9-03b6-442d-a1a1-76068329c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=compression_retriever)\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248bbd6-d5c9-403a-a4de-69e96bd8b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. EmbeddingsFilter\n",
    "# Filter out all chunks that are less similar to user query embedding\n",
    "# https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter.html\n",
    "\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=bge_embeddings, k=5)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "\n",
    "pprint(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2796e0-c6b6-4820-8513-1ebf8f6fd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline using several Filters\n",
    "\n",
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "## Creating the pipeline\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "\n",
    "    # 1. Extract important info wrt query\n",
    "    # 2. Filter based on query relevance\n",
    "    # 3. Filter based on embedding redundancy\n",
    "    transformers=[#CharacterTextSplitter(chunk_size=200, chunk_overlap=0, separator=\". \"), # first split chunks\n",
    "                  LLMChainExtractor.from_llm(llm), # extract useful information from these chunks\n",
    "                  LLMChainFilter.from_llm(llm), # filter these texts based on query relevance\n",
    "                  EmbeddingsRedundantFilter(embeddings=bge_embeddings, similarity_threshold=0.90), # filter on emb similarity\n",
    "                  #EmbeddingsFilter(embeddings=bge_embeddings, k=3), # get 3 most relavant chunks by similarity to query\n",
    "                 ]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor,\n",
    "                                                       base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "pprint(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f17b0-6d0a-41e4-ad23-81f69c5df1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=compression_retriever)\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc4183b-4567-4d39-9c94-e33f166dd5a4",
   "metadata": {},
   "source": [
    "Examples Pipelines\n",
    "\n",
    "Example 1 - filter, rewrite, check with embeddings\n",
    "\n",
    "Example 2 - retrieve multiple sources (Ensemble with BM25), filter, rewrite,\n",
    "\n",
    "Example 3 - retrieve, split, check splits with embeddings, filter, rewrite,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0381f0-07d0-4d9b-baea-ceecf142bd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd875da-ba3d-4f25-ad82-9a6f13555f09",
   "metadata": {},
   "source": [
    "### MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539e830-670e-4be5-bc76-5bbcb1004fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "## Text Splitting & Docloader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "import uuid\n",
    "import glob\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    #model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "def pprint(docs):\n",
    "    print(f\"\\n{'-' * 50}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "    \n",
    "loaders = [\n",
    "    TextLoader(doc, encoding=\"utf-8\") for doc in glob.glob(\"../resources/tests/document_store/*.txt\")\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for l in loaders:\n",
    "    docs.extend(l.load())\n",
    "    \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# collection_name has to be different in each .from_documents call, otherwise Chroma always uses default collection\n",
    "retriever = Chroma.from_documents(chunks, collection_name=str(uuid.uuid4()), embedding=bge_embeddings)\\\n",
    "                    .as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cbec8c-3502-4b7d-aae8-2b090436792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "query = \"What did Churchill think?\"\n",
    "\n",
    "docs = multi_query_retriever.get_relevant_documents(query=query)\n",
    "\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001fcbf-955c-4afb-81b5-cd0df1cb6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(), override=True)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=multi_query_retriever,\n",
    "                                 return_source_documents=True)\n",
    "\n",
    "qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af094945-689a-4c75-9c00-269f23d0fafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1772a351-c868-445d-a13b-b7d4dad2b882",
   "metadata": {},
   "source": [
    "### Cohere Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364a889-d968-48bd-b066-b5f963594310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32b995-e995-40ae-b8dd-8dbaa2388d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 50}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da0148-57a5-4311-8f60-6b5b7ec0749d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "import glob\n",
    "import uuid\n",
    "\n",
    "documents = TextLoader(glob.glob(\"../resources/tests/document_store/*.txt\")[0], encoding=\"utf-8\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=10)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "retriever = Chroma.from_documents(texts, collection_name=str(uuid.uuid4()), embedding=OpenAIEmbeddings()).as_retriever(\n",
    "    search_kwargs={\"k\": 20}\n",
    ")\n",
    "\n",
    "query = \"What did Churchill think?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2235d-75a6-47f2-91ca-4cf749fdcfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = CohereRerank()\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "\n",
    "pprint(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28fec43-5a8a-4bd2-8b12-81005505c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0), retriever=compression_retriever\n",
    ")\n",
    "\n",
    "chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594665c-0296-4028-a052-22e118b1ebfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135d587-24bb-4faa-8c53-03ed5f5f2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With MultiQueryRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=CohereRerank(), base_retriever=MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    ")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0), retriever=multi_reranker\n",
    ")\n",
    "\n",
    "chain({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ed506-52e3-479e-9f5b-ecceadc1fefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa831a-9e47-49a1-8123-39feff0cf3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf341704-d232-4c65-be91-9133040a3ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
